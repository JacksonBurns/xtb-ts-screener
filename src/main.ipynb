{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `XTBTSScreener.jl`\n",
    "## Screening Likely Transition States with Julia and Machine Learning\n",
    "This Jupyter notebook demonstrates the use of machine learning to predict if a partially-optimized initialization of a transition state, used in the study of chemical kinetics to predict rate constants, is _like to converge\"_ and produze a valid transition state or not after further simulation with expensive Density Functional Theory simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Lux, Random, Optimisers, Zygote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskLocalRNG()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Seeding\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(rng, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(\n",
       "    layer_1 = BatchNorm(128, affine=true, track_stats=true),  \u001b[90m# 256 parameters\u001b[39m\u001b[90m, plus 257\u001b[39m\n",
       "    layer_2 = Dense(128 => 256, tanh_fast),  \u001b[90m# 33_024 parameters\u001b[39m\n",
       "    layer_3 = BatchNorm(256, affine=true, track_stats=true),  \u001b[90m# 512 parameters\u001b[39m\u001b[90m, plus 513\u001b[39m\n",
       "    layer_4 = Dense(256 => 1, tanh_fast),  \u001b[90m# 257 parameters\u001b[39m\n",
       "    layer_5 = Dense(1 => 10),           \u001b[90m# 20 parameters\u001b[39m\n",
       ") \u001b[90m        # Total: \u001b[39m34_069 parameters,\n",
       "\u001b[90m          #        plus \u001b[39m770 states, \u001b[90msummarysize \u001b[39m80 bytes."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Construct the layer\n",
    "model = Chain(\n",
    "    BatchNorm(128),\n",
    "    Dense(128, 256, tanh),\n",
    "    BatchNorm(256),\n",
    "    Chain(Dense(256, 1, tanh),\n",
    "    Dense(1, 10)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((layer_1 = (scale = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], bias = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), layer_2 = (weight = Float32[-0.11034693 0.10973185 … 0.097955346 -0.009067461; -0.0111903995 0.07578978 … -0.03190492 0.08886787; … ; 0.01854451 -0.035003364 … -0.016294405 0.019076452; -0.09206565 -0.047390625 … -0.08859007 0.009517342], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_3 = (scale = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], bias = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), layer_4 = (weight = Float32[0.05381791 -0.103856824 … -0.050962884 0.020612676], bias = Float32[0.0;;]), layer_5 = (weight = Float32[-0.65478534; 0.61009777; … ; 0.41110995; 0.5493141;;], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;])), (layer_1 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], running_var = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = NamedTuple()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parameter and State Variables\n",
    "ps, st = Lux.setup(rng, model) #.|> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128×2 Matrix{Float32}:\n",
       " 0.188564   0.4228\n",
       " 0.683095   0.953174\n",
       " 0.0598976  0.62799\n",
       " 0.677622   0.564635\n",
       " 0.0432115  0.228648\n",
       " 0.645642   0.533853\n",
       " 0.709369   0.0650043\n",
       " 0.634036   0.0942084\n",
       " 0.639628   0.828258\n",
       " 0.559584   0.347723\n",
       " ⋮          \n",
       " 0.870554   0.349935\n",
       " 0.669238   0.635986\n",
       " 0.504906   0.741774\n",
       " 0.494614   0.238266\n",
       " 0.951539   0.450495\n",
       " 0.0595562  0.402075\n",
       " 0.746626   0.212307\n",
       " 0.884608   0.239166\n",
       " 0.687504   0.82052"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dummy Input\n",
    "x = rand(rng, Float32, 128, 2) #|> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Float32[0.63368976 -0.63368976; -0.59044194 0.59044194; … ; -0.39786503 0.39786503; -0.53161657 0.53161657], (layer_1 = (running_mean = Float32[0.03056821, 0.08181342, 0.03439437, 0.062112845, 0.013592961, 0.05897472, 0.038718663, 0.036412235, 0.07339431, 0.04536539  …  0.045826413, 0.061024453, 0.06526123, 0.06233399, 0.03664399, 0.0701017, 0.02308154, 0.047946673, 0.05618869, 0.07540121], running_var = Float32[0.9027433, 0.9036471, 0.9161364, 0.9006383, 0.90171933, 0.9006248, 0.9207603, 0.9145707, 0.90177906, 0.9022442  …  0.9199226, 0.9135522, 0.9000553, 0.90280527, 0.9032857, 0.9125522, 0.9058659, 0.9142748, 0.9208297, 0.9008846], training = Val{true}()), layer_2 = NamedTuple(), layer_3 = (running_mean = Float32[2.8312206f-8, 3.576279f-8, 2.9802322f-8, -1.7881394f-8, 5.3644182f-8, 0.0, -8.791685f-8, 8.940697f-9, -2.9802323f-9, -2.5331975f-8  …  2.0861625f-8, -2.3841858f-8, 8.34465f-8, 3.8556756f-8, -8.0093745f-9, 4.172325f-8, 1.3411045f-7, -1.5199184f-7, 7.897616f-8, -4.4703484f-8], running_var = Float32[0.93488127, 1.005513, 0.95133317, 1.0509164, 0.9091387, 1.0085077, 0.9431596, 1.0807321, 0.9547293, 0.919535  …  1.0275776, 1.0782654, 0.9504044, 0.90046877, 0.9007233, 0.9894871, 0.92271554, 0.90623885, 0.918961, 0.92120796], training = Val{true}()), layer_4 = NamedTuple(), layer_5 = NamedTuple()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the model\n",
    "y, st = Lux.apply(model, x, ps, st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(layer_1 = (scale = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], bias = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), layer_2 = (weight = Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_3 = (scale = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], bias = Float32[0.008653244, -0.016698875, 0.023558283, 5.5808847f-5, -0.018153034, -0.023646543, -0.0073172706, 0.017697593, 0.011687537, 0.0076471795  …  -0.016878096, 0.009687504, -0.007905553, 0.018811973, 0.00070813333, -0.019242885, 0.001841197, 0.0038661156, -0.008194192, 0.0033142595]), layer_4 = (weight = Float32[-2.6453324f-9 7.155023f-10 … -1.3974826f-9 2.0005824f-9], bias = Float32[0.16078745;;]), layer_5 = (weight = Float32[0.0; 0.0; … ; 0.0; 0.0;;], bias = Float32[2.0; 2.0; … ; 2.0; 2.0;;]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gradients\n",
    "## Pullback API to capture change in state\n",
    "(l, st_), pb = pullback(p -> Lux.apply(model, x, p, st), ps)\n",
    "gs = pb((one.(l), nothing))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((layer_1 = (scale = \u001b[32mLeaf(Adam{Float64}(0.0001, (0.9, 0.999), 2.22045e-16), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam{Float64}(0.0001, (0.9, 0.999), 2.22045e-16), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.81, 0.998001))\u001b[32m)\u001b[39m), layer_2 = (weight = \u001b[32mLeaf(Adam{Float64}(0.0001, (0.9, 0.999), 2.22045e-16), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam{Float64}(0.0001, (0.9, 0.999), 2.22045e-16), \u001b[39m(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.81, 0.998001))\u001b[32m)\u001b[39m), layer_3 = (scale = \u001b[32mLeaf(Adam{Float64}(0.0001, (0.9, 0.999), 2.22045e-16), \u001b[39m(Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam{Float64}(0.0001, (0.9, 0.999), 2.22045e-16), \u001b[39m(Float32[0.000865325, -0.00166989, 0.00235583, 5.58089f-6, -0.0018153, -0.00236465, -0.000731727, 0.00176976, 0.00116875, 0.000764718  …  -0.00168781, 0.000968751, -0.000790555, 0.0018812, 7.08134f-5, -0.00192429, 0.00018412, 0.000386612, -0.000819419, 0.000331426], Float32[7.48777f-8, 2.78849f-7, 5.54986f-7, 3.11459f-12, 3.29528f-7, 5.59152f-7, 5.35418f-8, 3.13201f-7, 1.36597f-7, 5.84786f-8  …  2.84866f-7, 9.38465f-8, 6.2497f-8, 3.53886f-7, 5.01446f-10, 3.70284f-7, 3.38996f-9, 1.49467f-8, 6.71439f-8, 1.09842f-8], (0.81, 0.998001))\u001b[32m)\u001b[39m), layer_4 = (weight = \u001b[32mLeaf(Adam{Float64}(0.0001, (0.9, 0.999), 2.22045e-16), \u001b[39m(Float32[-2.64533f-10 7.15503f-11 … -1.39748f-10 2.00058f-10], Float32[6.99769f-21 5.11937f-22 … 1.95293f-21 4.00228f-21], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam{Float64}(0.0001, (0.9, 0.999), 2.22045e-16), \u001b[39m(Float32[0.0160787;;], Float32[2.58523f-5;;], (0.81, 0.998001))\u001b[32m)\u001b[39m), layer_5 = (weight = \u001b[32mLeaf(Adam{Float64}(0.0001, (0.9, 0.999), 2.22045e-16), \u001b[39m(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.81, 0.998001))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam{Float64}(0.0001, (0.9, 0.999), 2.22045e-16), \u001b[39m(Float32[0.2; 0.2; … ; 0.2; 0.2;;], Float32[0.00399995; 0.00399995; … ; 0.00399995; 0.00399995;;], (0.81, 0.998001))\u001b[32m)\u001b[39m)), (layer_1 = (scale = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], bias = Float32[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), layer_2 = (weight = Float32[-0.11034693 0.10973185 … 0.097955346 -0.009067461; -0.0111903995 0.07578978 … -0.03190492 0.08886787; … ; 0.01854451 -0.035003364 … -0.016294405 0.019076452; -0.09206565 -0.047390625 … -0.08859007 0.009517342], bias = Float32[0.0; 0.0; … ; 0.0; 0.0;;]), layer_3 = (scale = Float32[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], bias = Float32[-0.000100000005, 0.0001, -0.000100000005, -0.0001, 0.0001, 0.0001, 0.0001, -0.0001, -0.0001, -0.000100000005  …  0.000100000005, -0.0001, 0.0001, -0.0001, -0.000100000005, 0.0001, -0.000100000005, -0.0001, 0.000100000005, -0.000100000005]), layer_4 = (weight = Float32[0.05391791 -0.103956826 … -0.050862882 0.020512676], bias = Float32[-0.0001;;]), layer_5 = (weight = Float32[-0.65478534; 0.61009777; … ; 0.41110995; 0.5493141;;], bias = Float32[-0.0001; -0.0001; … ; -0.0001; -0.0001;;])))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optimization\n",
    "st_opt = Optimisers.setup(Optimisers.ADAM(0.0001), ps)\n",
    "st_opt, ps = Optimisers.update(st_opt, ps, gs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
