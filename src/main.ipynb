{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `XTBTSScreener.jl` - Screening Likely Transition States with Julia and Machine Learning\n",
    "This Jupyter notebook demonstrates the use of machine learning to predict if a partially-optimized initialization of a transition state, used in the study of chemical kinetics to predict rate constants, is _like to converge\"_ and produze a valid transition state or not after further simulation with expensive Density Functional Theory simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "The input data is saved in a CSV file, load it using `CSV.jl` and then partition the data into training and testing sets using `MLUtils.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLUtils, CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_dataloaders (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_dataloaders()\n",
    "    csv_reader = CSV.File(\"data/co2_data.csv\")\n",
    "    n_samples = 313\n",
    "    x_data = Array{Float32}(undef, 55, 6, n_samples)\n",
    "    labels = Float32[]\n",
    "    iter = 1\n",
    "    for row in csv_reader[1:n_samples]\n",
    "        # get if it converged or not\n",
    "        if parse(Bool, \"$(row.converged)\")\n",
    "            push!(labels, 1.0f0)\n",
    "        else\n",
    "            push!(labels, 0.0f0)\n",
    "        end\n",
    "\n",
    "        # get the final coordinates of the atoms\n",
    "        split_array = split(\"$(row.xyz)\")\n",
    "        arr_end = length(split_array)\n",
    "        final_step = findlast( x -> occursin(\"[[\", x), split_array)\n",
    "        n_atoms = Int(length(split_array[final_step:arr_end])/6)\n",
    "        m = Array{Float32}(undef, 55, 6)\n",
    "        row_counter = 1\n",
    "        column_counter = 1\n",
    "        for value in split_array[final_step:arr_end]\n",
    "            temp = String(value)\n",
    "            temp = replace(temp,\"]\"=>\"\")\n",
    "            temp = replace(temp,\"[\"=>\"\")\n",
    "            temp = replace(temp,\",\"=>\"\")\n",
    "            m[row_counter, column_counter] = parse(Float32, temp)\n",
    "            column_counter += 1\n",
    "            if column_counter > 6\n",
    "                column_counter = 1\n",
    "                row_counter += 1\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # zero-padding\n",
    "        for i in n_atoms+1:55\n",
    "            m[i, 1:6] = [0,0,0,0,0,0]\n",
    "        end\n",
    "        x_data[1:55, 1:6, iter] = m\n",
    "        iter += 1\n",
    "    end\n",
    "    (x_train, y_train), (x_val, y_val) = splitobs((x_data, labels); at=0.8, shuffle=true)\n",
    "    return (DataLoader(collect.((x_train, y_train)); batchsize=2^6, shuffle=true),\n",
    "            DataLoader(collect.((x_val, y_val)); batchsize=2^6, shuffle=false))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_tutorial_dataloaders (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_tutorial_dataloaders()\n",
    "    dataset_size=1000\n",
    "    sequence_length=50\n",
    "    data = [MLUtils.Datasets.make_spiral(sequence_length) for _ in 1:dataset_size]\n",
    "    # Get the labels\n",
    "    labels = vcat(repeat([0.0f0], dataset_size ÷ 2), repeat([1.0f0], dataset_size ÷ 2))\n",
    "    clockwise_spirals = [reshape(d[1][:, 1:sequence_length], :, sequence_length, 1)\n",
    "                         for d in data[1:(dataset_size ÷ 2)]]\n",
    "    anticlockwise_spirals = [reshape(d[1][:, (sequence_length + 1):end], :, sequence_length,\n",
    "                                     1) for d in data[((dataset_size ÷ 2) + 1):end]]\n",
    "    x_data = Float32.(cat(clockwise_spirals..., anticlockwise_spirals...; dims=3))\n",
    "    # Split the dataset\n",
    "    (x_train, y_train), (x_val, y_val) = splitobs((x_data, labels); at=0.8, shuffle=true)\n",
    "    return (DataLoader(collect.((x_train, y_train)); batchsize=128, shuffle=true),\n",
    "            DataLoader(collect.((x_val, y_val)); batchsize=128, shuffle=false))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Neural Network\n",
    "Following from the tutorial in the [Lux documentation](https://lux.csail.mit.edu/stable/examples/generated/beginner/SimpleRNN/main/) we write a series of functions that will create our NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Lux, Random, Optimisers, Zygote, NNlib, Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskLocalRNG()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seeding\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(rng, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct StateClassifier{L, C} <:\n",
    "       Lux.AbstractExplicitContainerLayer{(:lstm_cell, :classifier)}\n",
    "    lstm_cell::L\n",
    "    classifier::C\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateClassifier"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function StateClassifier(in_dims, hidden_dims, out_dims)\n",
    "    return StateClassifier(LSTMCell(in_dims => hidden_dims),\n",
    "                            Dense(hidden_dims => out_dims, sigmoid))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (s::StateClassifier)(x::AbstractArray{T, 3}, ps::NamedTuple,\n",
    "                               st::NamedTuple) where {T}\n",
    "    x_init, x_rest = Iterators.peel(eachslice(x; dims=2))\n",
    "    (y, carry), st_lstm = s.lstm_cell(x_init, ps.lstm_cell, st.lstm_cell)\n",
    "    for x in x_rest\n",
    "        (y, carry), st_lstm = s.lstm_cell((x, carry), ps.lstm_cell, st_lstm)\n",
    "    end\n",
    "    y, st_classifier = s.classifier(y, ps.classifier, st.classifier)\n",
    "    st = merge(st, (classifier=st_classifier, lstm_cell=st_lstm))\n",
    "    return vec(y), st\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function xlogy(x, y)\n",
    "    result = x * log(y)\n",
    "    return ifelse(iszero(x), zero(result), result)\n",
    "end\n",
    "\n",
    "function binarycrossentropy(y_pred, y_true)\n",
    "    y_pred = y_pred .+ eps(eltype(y_pred))\n",
    "    return mean(@. -xlogy(y_true, y_pred) - xlogy(1 - y_true, 1 - y_pred))\n",
    "end\n",
    "\n",
    "function compute_loss(x, y, model, ps, st)\n",
    "    y_pred, st = model(x, ps, st)\n",
    "    return binarycrossentropy(y_pred, y), y_pred, st\n",
    "end\n",
    "\n",
    "matches(y_pred, y_true) = sum((y_pred .> 0.5) .== y_true)\n",
    "accuracy(y_pred, y_true) = matches(y_pred, y_true) / length(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_optimiser (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function create_optimiser(ps)\n",
    "    opt = Optimisers.ADAM(0.01f0)\n",
    "    return Optimisers.setup(opt, ps)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the NN\n",
    "Actual training and evaluation steps.\n",
    "\n",
    "Load the data from the file and parition it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DataLoader(::Tuple{Array{Float32, 3}, Vector{Float32}}, shuffle=true, batchsize=64), DataLoader(::Tuple{Array{Float32, 3}, Vector{Float32}}, batchsize=64))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_loader, val_loader) = get_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model and optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(lstm_cell = (weight_i = \u001b[32mLeaf(Adam{Float32}(0.01, (0.9, 0.999), 1.19209f-7), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, weight_h = \u001b[32mLeaf(Adam{Float32}(0.01, (0.9, 0.999), 1.19209f-7), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam{Float32}(0.01, (0.9, 0.999), 1.19209f-7), \u001b[39m(Float32[0.0; 0.0; … ; 0.0; 0.0;;], Float32[0.0; 0.0; … ; 0.0; 0.0;;], (0.9, 0.999))\u001b[32m)\u001b[39m), classifier = (weight = \u001b[32mLeaf(Adam{Float32}(0.01, (0.9, 0.999), 1.19209f-7), \u001b[39m(Float32[0.0 0.0 … 0.0 0.0], Float32[0.0 0.0 … 0.0 0.0], (0.9, 0.999))\u001b[32m)\u001b[39m, bias = \u001b[32mLeaf(Adam{Float32}(0.01, (0.9, 0.999), 1.19209f-7), \u001b[39m(Float32[0.0;;], Float32[0.0;;], (0.9, 0.999))\u001b[32m)\u001b[39m))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = StateClassifier(55, 6, 1)\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(rng, 0)\n",
    "ps, st = Lux.setup(rng, model)\n",
    "opt_state = create_optimiser(ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual model training and validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1]: Loss 0.65877956\n",
      "Epoch [1]: Loss 0.5727344\n",
      "Epoch [1]: Loss 0.6015013\n",
      "Epoch [1]: Loss 0.6137502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: Loss 0.53537816 Accuracy 0.7619047619047619\n",
      "Epoch [2]: Loss 0.53607404\n",
      "Epoch [2]: Loss 0.5209814\n",
      "Epoch [2]: Loss 0.51971835\n",
      "Epoch [2]: Loss 0.56570756\n",
      "Validation: Loss 0.49420032 Accuracy 0.7936507936507936\n",
      "Epoch [3]: Loss 0.49104878\n",
      "Epoch [3]: Loss 0.4777782\n",
      "Epoch [3]: Loss 0.5371938\n",
      "Epoch [3]: Loss 0.47244236\n",
      "Validation: Loss 0.48514286 Accuracy 0.8253968253968254\n",
      "Epoch [4]: Loss 0.49536335\n",
      "Epoch [4]: Loss 0.41711527\n",
      "Epoch [4]: Loss 0.44593868\n",
      "Epoch [4]: Loss 0.46768925\n",
      "Validation: Loss 0.4797901 Accuracy 0.8095238095238095\n",
      "Epoch [5]: Loss 0.4729952\n",
      "Epoch [5]: Loss 0.43468007\n",
      "Epoch [5]: Loss 0.3683915\n",
      "Epoch [5]: Loss 0.44661713\n",
      "Validation: Loss 0.47465354 Accuracy 0.8095238095238095\n",
      "Epoch [6]: Loss 0.44903395\n",
      "Epoch [6]: Loss 0.3968253\n",
      "Epoch [6]: Loss 0.48466167\n",
      "Epoch [6]: Loss 0.2777892\n",
      "Validation: Loss 0.47269198 Accuracy 0.8095238095238095\n",
      "Epoch [7]: Loss 0.39031193\n",
      "Epoch [7]: Loss 0.36755762\n",
      "Epoch [7]: Loss 0.38665572\n",
      "Epoch [7]: Loss 0.37399495\n",
      "Validation: Loss 0.47289717 Accuracy 0.8095238095238095\n",
      "Epoch [8]: Loss 0.4399644\n",
      "Epoch [8]: Loss 0.31964463\n",
      "Epoch [8]: Loss 0.29891846\n",
      "Epoch [8]: Loss 0.3714343\n",
      "Validation: Loss 0.47663012 Accuracy 0.8095238095238095\n",
      "Epoch [9]: Loss 0.34139186\n",
      "Epoch [9]: Loss 0.28378454\n",
      "Epoch [9]: Loss 0.32694155\n",
      "Epoch [9]: Loss 0.38541952\n",
      "Validation: Loss 0.47825354 Accuracy 0.8095238095238095\n",
      "Epoch [10]: Loss 0.26863495\n",
      "Epoch [10]: Loss 0.3797444\n",
      "Epoch [10]: Loss 0.29582548\n",
      "Epoch [10]: Loss 0.2966826\n",
      "Validation: Loss 0.48182124 Accuracy 0.8095238095238095\n",
      "Epoch [11]: Loss 0.32237363\n",
      "Epoch [11]: Loss 0.3217601\n",
      "Epoch [11]: Loss 0.2046945\n",
      "Epoch [11]: Loss 0.30290902\n",
      "Validation: Loss 0.4823881 Accuracy 0.8253968253968254\n",
      "Epoch [12]: Loss 0.29706672\n",
      "Epoch [12]: Loss 0.2409125\n",
      "Epoch [12]: Loss 0.25086522\n",
      "Epoch [12]: Loss 0.29362705\n",
      "Validation: Loss 0.48480493 Accuracy 0.8095238095238095\n",
      "Epoch [13]: Loss 0.28187647\n",
      "Epoch [13]: Loss 0.28482577\n",
      "Epoch [13]: Loss 0.19339336\n",
      "Epoch [13]: Loss 0.2407813\n",
      "Validation: Loss 0.48856783 Accuracy 0.8095238095238095\n",
      "Epoch [14]: Loss 0.29630655\n",
      "Epoch [14]: Loss 0.20929901\n",
      "Epoch [14]: Loss 0.24307021\n",
      "Epoch [14]: Loss 0.18069567\n",
      "Validation: Loss 0.49546352 Accuracy 0.8253968253968254\n",
      "Epoch [15]: Loss 0.19427407\n",
      "Epoch [15]: Loss 0.1762525\n",
      "Epoch [15]: Loss 0.26439235\n",
      "Epoch [15]: Loss 0.23101346\n",
      "Validation: Loss 0.4962737 Accuracy 0.8253968253968254\n",
      "Epoch [16]: Loss 0.21045035\n",
      "Epoch [16]: Loss 0.19285513\n",
      "Epoch [16]: Loss 0.18965629\n",
      "Epoch [16]: Loss 0.20513976\n",
      "Validation: Loss 0.4952405 Accuracy 0.8253968253968254\n",
      "Epoch [17]: Loss 0.13640134\n",
      "Epoch [17]: Loss 0.20897439\n",
      "Epoch [17]: Loss 0.21332946\n",
      "Epoch [17]: Loss 0.17325862\n",
      "Validation: Loss 0.5005995 Accuracy 0.8253968253968254\n",
      "Epoch [18]: Loss 0.1562526\n",
      "Epoch [18]: Loss 0.18916059\n",
      "Epoch [18]: Loss 0.16387329\n",
      "Epoch [18]: Loss 0.17008218\n",
      "Validation: Loss 0.5047327 Accuracy 0.8253968253968254\n",
      "Epoch [19]: Loss 0.1274913\n",
      "Epoch [19]: Loss 0.15200645\n",
      "Epoch [19]: Loss 0.18553847\n",
      "Epoch [19]: Loss 0.15244219\n",
      "Validation: Loss 0.5033818 Accuracy 0.8253968253968254\n",
      "Epoch [20]: Loss 0.09479004\n",
      "Epoch [20]: Loss 0.18813486\n",
      "Epoch [20]: Loss 0.11712545\n",
      "Epoch [20]: Loss 0.16658622\n",
      "Validation: Loss 0.49608183 Accuracy 0.8253968253968254\n",
      "Epoch [21]: Loss 0.11719336\n",
      "Epoch [21]: Loss 0.16765429\n",
      "Epoch [21]: Loss 0.11955314\n",
      "Epoch [21]: Loss 0.1174206\n",
      "Validation: Loss 0.5072411 Accuracy 0.8095238095238095\n",
      "Epoch [22]: Loss 0.13727064\n",
      "Epoch [22]: Loss 0.13711421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22]: Loss 0.097937286\n",
      "Epoch [22]: Loss 0.10929225\n",
      "Validation: Loss 0.522 Accuracy 0.8253968253968254\n",
      "Epoch [23]: Loss 0.10519026\n",
      "Epoch [23]: Loss 0.16066696\n",
      "Epoch [23]: Loss 0.07705802\n",
      "Epoch [23]: Loss 0.1025305\n",
      "Validation: Loss 0.525282 Accuracy 0.8253968253968254\n",
      "Epoch [24]: Loss 0.09746854\n",
      "Epoch [24]: Loss 0.0937297\n",
      "Epoch [24]: Loss 0.10134173\n",
      "Epoch [24]: Loss 0.12407709\n",
      "Validation: Loss 0.5284035 Accuracy 0.8253968253968254\n",
      "Epoch [25]: Loss 0.114411585\n",
      "Epoch [25]: Loss 0.08495301\n",
      "Epoch [25]: Loss 0.07760929\n",
      "Epoch [25]: Loss 0.114254735\n",
      "Validation: Loss 0.54022175 Accuracy 0.8253968253968254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mNumber of observations less than batch-size, decreasing the batch-size to 63\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ MLUtils ~/.julia/packages/MLUtils/R44Zf/src/batchview.jl:95\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "loss_vector = Float64[]\n",
    "for epoch in 1:25\n",
    "    # Train the model\n",
    "    for (x, y) in train_loader\n",
    "        (loss, y_pred, st), back = pullback(p -> compute_loss(x, y, model, p, st), ps)\n",
    "        gs = back((one(loss), nothing, nothing))[1]\n",
    "        opt_state, ps = Optimisers.update(opt_state, ps, gs)\n",
    "\n",
    "        println(\"Epoch [$epoch]: Loss $loss\")\n",
    "        push!(loss_vector, loss)\n",
    "    end\n",
    "\n",
    "    # Validate the model\n",
    "    st_ = Lux.testmode(st)\n",
    "    for (x, y) in val_loader\n",
    "        (loss, y_pred, st_) = compute_loss(x, y, model, ps, st_)\n",
    "        acc = accuracy(y_pred, y)\n",
    "        println(\"Validation: Loss $loss Accuracy $acc\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip480\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip480)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip481\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip480)\" d=\"\n",
       "M156.598 1486.45 L2352.76 1486.45 L2352.76 47.2441 L156.598 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip482\">\n",
       "    <rect x=\"156\" y=\"47\" width=\"2197\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip482)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  197.826,1486.45 197.826,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip482)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  721.02,1486.45 721.02,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip482)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1244.21,1486.45 1244.21,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip482)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1767.41,1486.45 1767.41,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip482)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2290.6,1486.45 2290.6,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  197.826,1486.45 197.826,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  721.02,1486.45 721.02,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1244.21,1486.45 1244.21,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1767.41,1486.45 1767.41,1467.55 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2290.6,1486.45 2290.6,1467.55 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip480)\" d=\"M197.826 1517.37 Q194.215 1517.37 192.386 1520.93 Q190.581 1524.47 190.581 1531.6 Q190.581 1538.71 192.386 1542.27 Q194.215 1545.82 197.826 1545.82 Q201.46 1545.82 203.266 1542.27 Q205.094 1538.71 205.094 1531.6 Q205.094 1524.47 203.266 1520.93 Q201.46 1517.37 197.826 1517.37 M197.826 1513.66 Q203.636 1513.66 206.692 1518.27 Q209.77 1522.85 209.77 1531.6 Q209.77 1540.33 206.692 1544.94 Q203.636 1549.52 197.826 1549.52 Q192.016 1549.52 188.937 1544.94 Q185.882 1540.33 185.882 1531.6 Q185.882 1522.85 188.937 1518.27 Q192.016 1513.66 197.826 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M700.29 1544.91 L716.61 1544.91 L716.61 1548.85 L694.666 1548.85 L694.666 1544.91 Q697.328 1542.16 701.911 1537.53 Q706.517 1532.88 707.698 1531.53 Q709.943 1529.01 710.823 1527.27 Q711.726 1525.51 711.726 1523.82 Q711.726 1521.07 709.781 1519.33 Q707.86 1517.6 704.758 1517.6 Q702.559 1517.6 700.105 1518.36 Q697.675 1519.13 694.897 1520.68 L694.897 1515.95 Q697.721 1514.82 700.175 1514.24 Q702.628 1513.66 704.665 1513.66 Q710.036 1513.66 713.23 1516.35 Q716.425 1519.03 716.425 1523.52 Q716.425 1525.65 715.614 1527.57 Q714.827 1529.47 712.721 1532.07 Q712.142 1532.74 709.04 1535.95 Q705.939 1539.15 700.29 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M726.471 1514.29 L744.827 1514.29 L744.827 1518.22 L730.753 1518.22 L730.753 1526.7 Q731.772 1526.35 732.79 1526.19 Q733.809 1526 734.827 1526 Q740.614 1526 743.994 1529.17 Q747.374 1532.34 747.374 1537.76 Q747.374 1543.34 743.901 1546.44 Q740.429 1549.52 734.11 1549.52 Q731.934 1549.52 729.665 1549.15 Q727.42 1548.78 725.013 1548.04 L725.013 1543.34 Q727.096 1544.47 729.318 1545.03 Q731.54 1545.58 734.017 1545.58 Q738.022 1545.58 740.36 1543.48 Q742.698 1541.37 742.698 1537.76 Q742.698 1534.15 740.36 1532.04 Q738.022 1529.94 734.017 1529.94 Q732.142 1529.94 730.267 1530.35 Q728.415 1530.77 726.471 1531.65 L726.471 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M1218.91 1514.29 L1237.27 1514.29 L1237.27 1518.22 L1223.19 1518.22 L1223.19 1526.7 Q1224.21 1526.35 1225.23 1526.19 Q1226.25 1526 1227.27 1526 Q1233.06 1526 1236.44 1529.17 Q1239.82 1532.34 1239.82 1537.76 Q1239.82 1543.34 1236.34 1546.44 Q1232.87 1549.52 1226.55 1549.52 Q1224.38 1549.52 1222.11 1549.15 Q1219.86 1548.78 1217.45 1548.04 L1217.45 1543.34 Q1219.54 1544.47 1221.76 1545.03 Q1223.98 1545.58 1226.46 1545.58 Q1230.46 1545.58 1232.8 1543.48 Q1235.14 1541.37 1235.14 1537.76 Q1235.14 1534.15 1232.8 1532.04 Q1230.46 1529.94 1226.46 1529.94 Q1224.58 1529.94 1222.71 1530.35 Q1220.86 1530.77 1218.91 1531.65 L1218.91 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M1259.03 1517.37 Q1255.42 1517.37 1253.59 1520.93 Q1251.78 1524.47 1251.78 1531.6 Q1251.78 1538.71 1253.59 1542.27 Q1255.42 1545.82 1259.03 1545.82 Q1262.66 1545.82 1264.47 1542.27 Q1266.3 1538.71 1266.3 1531.6 Q1266.3 1524.47 1264.47 1520.93 Q1262.66 1517.37 1259.03 1517.37 M1259.03 1513.66 Q1264.84 1513.66 1267.89 1518.27 Q1270.97 1522.85 1270.97 1531.6 Q1270.97 1540.33 1267.89 1544.94 Q1264.84 1549.52 1259.03 1549.52 Q1253.22 1549.52 1250.14 1544.94 Q1247.08 1540.33 1247.08 1531.6 Q1247.08 1522.85 1250.14 1518.27 Q1253.22 1513.66 1259.03 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M1741.26 1514.29 L1763.48 1514.29 L1763.48 1516.28 L1750.94 1548.85 L1746.05 1548.85 L1757.86 1518.22 L1741.26 1518.22 L1741.26 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M1772.65 1514.29 L1791.01 1514.29 L1791.01 1518.22 L1776.93 1518.22 L1776.93 1526.7 Q1777.95 1526.35 1778.97 1526.19 Q1779.99 1526 1781.01 1526 Q1786.79 1526 1790.17 1529.17 Q1793.55 1532.34 1793.55 1537.76 Q1793.55 1543.34 1790.08 1546.44 Q1786.61 1549.52 1780.29 1549.52 Q1778.11 1549.52 1775.84 1549.15 Q1773.6 1548.78 1771.19 1548.04 L1771.19 1543.34 Q1773.27 1544.47 1775.5 1545.03 Q1777.72 1545.58 1780.2 1545.58 Q1784.2 1545.58 1786.54 1543.48 Q1788.88 1541.37 1788.88 1537.76 Q1788.88 1534.15 1786.54 1532.04 Q1784.2 1529.94 1780.2 1529.94 Q1778.32 1529.94 1776.45 1530.35 Q1774.59 1530.77 1772.65 1531.65 L1772.65 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M2250.21 1544.91 L2257.85 1544.91 L2257.85 1518.55 L2249.54 1520.21 L2249.54 1515.95 L2257.8 1514.29 L2262.48 1514.29 L2262.48 1544.91 L2270.11 1544.91 L2270.11 1548.85 L2250.21 1548.85 L2250.21 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M2289.56 1517.37 Q2285.95 1517.37 2284.12 1520.93 Q2282.31 1524.47 2282.31 1531.6 Q2282.31 1538.71 2284.12 1542.27 Q2285.95 1545.82 2289.56 1545.82 Q2293.19 1545.82 2295 1542.27 Q2296.83 1538.71 2296.83 1531.6 Q2296.83 1524.47 2295 1520.93 Q2293.19 1517.37 2289.56 1517.37 M2289.56 1513.66 Q2295.37 1513.66 2298.42 1518.27 Q2301.5 1522.85 2301.5 1531.6 Q2301.5 1540.33 2298.42 1544.94 Q2295.37 1549.52 2289.56 1549.52 Q2283.75 1549.52 2280.67 1544.94 Q2277.61 1540.33 2277.61 1531.6 Q2277.61 1522.85 2280.67 1518.27 Q2283.75 1513.66 2289.56 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M2319.72 1517.37 Q2316.11 1517.37 2314.28 1520.93 Q2312.48 1524.47 2312.48 1531.6 Q2312.48 1538.71 2314.28 1542.27 Q2316.11 1545.82 2319.72 1545.82 Q2323.35 1545.82 2325.16 1542.27 Q2326.99 1538.71 2326.99 1531.6 Q2326.99 1524.47 2325.16 1520.93 Q2323.35 1517.37 2319.72 1517.37 M2319.72 1513.66 Q2325.53 1513.66 2328.59 1518.27 Q2331.67 1522.85 2331.67 1531.6 Q2331.67 1540.33 2328.59 1544.94 Q2325.53 1549.52 2319.72 1549.52 Q2313.91 1549.52 2310.83 1544.94 Q2307.78 1540.33 2307.78 1531.6 Q2307.78 1522.85 2310.83 1518.27 Q2313.91 1513.66 2319.72 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip482)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,1392.17 2352.76,1392.17 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip482)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,1158.77 2352.76,1158.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip482)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,925.369 2352.76,925.369 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip482)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,691.968 2352.76,691.968 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip482)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,458.568 2352.76,458.568 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip482)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  156.598,225.168 2352.76,225.168 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,1486.45 156.598,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,1392.17 175.496,1392.17 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,1158.77 175.496,1158.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,925.369 175.496,925.369 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,691.968 175.496,691.968 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,458.568 175.496,458.568 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  156.598,225.168 175.496,225.168 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip480)\" d=\"M64.6495 1377.97 Q61.0384 1377.97 59.2097 1381.53 Q57.4041 1385.07 57.4041 1392.2 Q57.4041 1399.31 59.2097 1402.88 Q61.0384 1406.42 64.6495 1406.42 Q68.2837 1406.42 70.0892 1402.88 Q71.9179 1399.31 71.9179 1392.2 Q71.9179 1385.07 70.0892 1381.53 Q68.2837 1377.97 64.6495 1377.97 M64.6495 1374.26 Q70.4596 1374.26 73.5152 1378.87 Q76.5938 1383.45 76.5938 1392.2 Q76.5938 1400.93 73.5152 1405.54 Q70.4596 1410.12 64.6495 1410.12 Q58.8393 1410.12 55.7606 1405.54 Q52.7051 1400.93 52.7051 1392.2 Q52.7051 1383.45 55.7606 1378.87 Q58.8393 1374.26 64.6495 1374.26 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M84.8114 1403.57 L89.6956 1403.57 L89.6956 1409.45 L84.8114 1409.45 L84.8114 1403.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M100.691 1405.51 L108.33 1405.51 L108.33 1379.15 L100.02 1380.81 L100.02 1376.56 L108.283 1374.89 L112.959 1374.89 L112.959 1405.51 L120.598 1405.51 L120.598 1409.45 L100.691 1409.45 L100.691 1405.51 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M65.0198 1144.57 Q61.4087 1144.57 59.58 1148.13 Q57.7745 1151.67 57.7745 1158.8 Q57.7745 1165.91 59.58 1169.47 Q61.4087 1173.02 65.0198 1173.02 Q68.6541 1173.02 70.4596 1169.47 Q72.2883 1165.91 72.2883 1158.8 Q72.2883 1151.67 70.4596 1148.13 Q68.6541 1144.57 65.0198 1144.57 M65.0198 1140.86 Q70.83 1140.86 73.8855 1145.47 Q76.9642 1150.05 76.9642 1158.8 Q76.9642 1167.53 73.8855 1172.14 Q70.83 1176.72 65.0198 1176.72 Q59.2097 1176.72 56.131 1172.14 Q53.0754 1167.53 53.0754 1158.8 Q53.0754 1150.05 56.131 1145.47 Q59.2097 1140.86 65.0198 1140.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M85.1818 1170.17 L90.066 1170.17 L90.066 1176.05 L85.1818 1176.05 L85.1818 1170.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M104.279 1172.11 L120.598 1172.11 L120.598 1176.05 L98.6539 1176.05 L98.6539 1172.11 Q101.316 1169.36 105.899 1164.73 Q110.506 1160.08 111.686 1158.73 Q113.932 1156.21 114.811 1154.47 Q115.714 1152.72 115.714 1151.03 Q115.714 1148.27 113.77 1146.54 Q111.848 1144.8 108.746 1144.8 Q106.547 1144.8 104.094 1145.56 Q101.663 1146.33 98.8854 1147.88 L98.8854 1143.16 Q101.709 1142.02 104.163 1141.44 Q106.617 1140.86 108.654 1140.86 Q114.024 1140.86 117.219 1143.55 Q120.413 1146.23 120.413 1150.72 Q120.413 1152.85 119.603 1154.78 Q118.816 1156.67 116.709 1159.27 Q116.131 1159.94 113.029 1163.16 Q109.927 1166.35 104.279 1172.11 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M64.0708 911.167 Q60.4597 911.167 58.631 914.732 Q56.8254 918.274 56.8254 925.403 Q56.8254 932.51 58.631 936.075 Q60.4597 939.616 64.0708 939.616 Q67.705 939.616 69.5105 936.075 Q71.3392 932.51 71.3392 925.403 Q71.3392 918.274 69.5105 914.732 Q67.705 911.167 64.0708 911.167 M64.0708 907.464 Q69.8809 907.464 72.9365 912.07 Q76.0151 916.653 76.0151 925.403 Q76.0151 934.13 72.9365 938.737 Q69.8809 943.32 64.0708 943.32 Q58.2606 943.32 55.1819 938.737 Q52.1264 934.13 52.1264 925.403 Q52.1264 916.653 55.1819 912.07 Q58.2606 907.464 64.0708 907.464 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M84.2327 936.769 L89.1169 936.769 L89.1169 942.649 L84.2327 942.649 L84.2327 936.769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M113.469 924.014 Q116.825 924.732 118.7 927.001 Q120.598 929.269 120.598 932.602 Q120.598 937.718 117.08 940.519 Q113.561 943.32 107.08 943.32 Q104.904 943.32 102.589 942.88 Q100.297 942.463 97.8437 941.607 L97.8437 937.093 Q99.7882 938.227 102.103 938.806 Q104.418 939.385 106.941 939.385 Q111.339 939.385 113.631 937.649 Q115.945 935.913 115.945 932.602 Q115.945 929.547 113.793 927.834 Q111.663 926.098 107.844 926.098 L103.816 926.098 L103.816 922.255 L108.029 922.255 Q111.478 922.255 113.307 920.889 Q115.135 919.501 115.135 916.908 Q115.135 914.246 113.237 912.834 Q111.362 911.399 107.844 911.399 Q105.922 911.399 103.723 911.815 Q101.524 912.232 98.8854 913.112 L98.8854 908.945 Q101.547 908.204 103.862 907.834 Q106.2 907.464 108.26 907.464 Q113.584 907.464 116.686 909.894 Q119.788 912.302 119.788 916.422 Q119.788 919.292 118.145 921.283 Q116.501 923.251 113.469 924.014 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M62.9365 677.767 Q59.3254 677.767 57.4967 681.332 Q55.6912 684.873 55.6912 692.003 Q55.6912 699.11 57.4967 702.674 Q59.3254 706.216 62.9365 706.216 Q66.5707 706.216 68.3763 702.674 Q70.205 699.11 70.205 692.003 Q70.205 684.873 68.3763 681.332 Q66.5707 677.767 62.9365 677.767 M62.9365 674.063 Q68.7467 674.063 71.8022 678.67 Q74.8809 683.253 74.8809 692.003 Q74.8809 700.73 71.8022 705.336 Q68.7467 709.92 62.9365 709.92 Q57.1264 709.92 54.0477 705.336 Q50.9921 700.73 50.9921 692.003 Q50.9921 683.253 54.0477 678.67 Q57.1264 674.063 62.9365 674.063 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M83.0984 703.369 L87.9827 703.369 L87.9827 709.248 L83.0984 709.248 L83.0984 703.369 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M111.015 678.762 L99.2095 697.211 L111.015 697.211 L111.015 678.762 M109.788 674.688 L115.668 674.688 L115.668 697.211 L120.598 697.211 L120.598 701.1 L115.668 701.1 L115.668 709.248 L111.015 709.248 L111.015 701.1 L95.4132 701.1 L95.4132 696.586 L109.788 674.688 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M64.418 444.367 Q60.8069 444.367 58.9782 447.932 Q57.1726 451.473 57.1726 458.603 Q57.1726 465.709 58.9782 469.274 Q60.8069 472.816 64.418 472.816 Q68.0522 472.816 69.8578 469.274 Q71.6865 465.709 71.6865 458.603 Q71.6865 451.473 69.8578 447.932 Q68.0522 444.367 64.418 444.367 M64.418 440.663 Q70.2281 440.663 73.2837 445.27 Q76.3624 449.853 76.3624 458.603 Q76.3624 467.33 73.2837 471.936 Q70.2281 476.519 64.418 476.519 Q58.6078 476.519 55.5291 471.936 Q52.4736 467.33 52.4736 458.603 Q52.4736 449.853 55.5291 445.27 Q58.6078 440.663 64.418 440.663 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M84.5799 469.969 L89.4641 469.969 L89.4641 475.848 L84.5799 475.848 L84.5799 469.969 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M99.6956 441.288 L118.052 441.288 L118.052 445.223 L103.978 445.223 L103.978 453.695 Q104.996 453.348 106.015 453.186 Q107.033 453.001 108.052 453.001 Q113.839 453.001 117.219 456.172 Q120.598 459.344 120.598 464.76 Q120.598 470.339 117.126 473.441 Q113.654 476.519 107.334 476.519 Q105.159 476.519 102.89 476.149 Q100.645 475.779 98.2372 475.038 L98.2372 470.339 Q100.321 471.473 102.543 472.029 Q104.765 472.584 107.242 472.584 Q111.246 472.584 113.584 470.478 Q115.922 468.371 115.922 464.76 Q115.922 461.149 113.584 459.043 Q111.246 456.936 107.242 456.936 Q105.367 456.936 103.492 457.353 Q101.64 457.77 99.6956 458.649 L99.6956 441.288 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M63.2606 210.967 Q59.6495 210.967 57.8208 214.531 Q56.0152 218.073 56.0152 225.203 Q56.0152 232.309 57.8208 235.874 Q59.6495 239.416 63.2606 239.416 Q66.8948 239.416 68.7004 235.874 Q70.5291 232.309 70.5291 225.203 Q70.5291 218.073 68.7004 214.531 Q66.8948 210.967 63.2606 210.967 M63.2606 207.263 Q69.0707 207.263 72.1263 211.869 Q75.205 216.453 75.205 225.203 Q75.205 233.929 72.1263 238.536 Q69.0707 243.119 63.2606 243.119 Q57.4504 243.119 54.3717 238.536 Q51.3162 233.929 51.3162 225.203 Q51.3162 216.453 54.3717 211.869 Q57.4504 207.263 63.2606 207.263 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M83.4225 236.568 L88.3067 236.568 L88.3067 242.448 L83.4225 242.448 L83.4225 236.568 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M109.071 223.304 Q105.922 223.304 104.071 225.457 Q102.242 227.61 102.242 231.36 Q102.242 235.087 104.071 237.263 Q105.922 239.416 109.071 239.416 Q112.219 239.416 114.047 237.263 Q115.899 235.087 115.899 231.36 Q115.899 227.61 114.047 225.457 Q112.219 223.304 109.071 223.304 M118.353 208.652 L118.353 212.911 Q116.594 212.078 114.788 211.638 Q113.006 211.198 111.246 211.198 Q106.617 211.198 104.163 214.323 Q101.733 217.448 101.385 223.767 Q102.751 221.754 104.811 220.689 Q106.871 219.601 109.348 219.601 Q114.557 219.601 117.566 222.772 Q120.598 225.92 120.598 231.36 Q120.598 236.684 117.45 239.902 Q114.302 243.119 109.071 243.119 Q103.075 243.119 99.9039 238.536 Q96.7326 233.929 96.7326 225.203 Q96.7326 217.008 100.621 212.147 Q104.51 207.263 111.061 207.263 Q112.82 207.263 114.603 207.61 Q116.408 207.957 118.353 208.652 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip482)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  218.754,87.9763 239.681,288.806 260.609,221.664 281.537,193.075 302.465,374.371 323.392,409.598 344.32,412.545 365.248,305.207 386.176,479.46 407.103,510.434 \n",
       "  428.031,371.758 448.959,522.888 469.887,469.39 490.814,652.021 511.742,584.747 532.67,533.982 553.598,521.597 574.525,611.025 595.453,765.743 616.381,583.164 \n",
       "  637.309,577.523 658.236,699.378 679.164,494.368 700.092,977.209 721.02,714.58 741.947,767.689 762.875,723.114 783.803,752.664 804.731,598.691 825.658,879.518 \n",
       "  846.586,927.893 867.514,758.641 888.442,828.76 909.369,963.216 930.297,862.487 951.225,725.999 972.152,998.575 993.08,739.245 1014.01,935.112 1034.94,933.111 \n",
       "  1055.86,873.148 1076.79,874.58 1097.72,1147.81 1118.65,918.579 1139.57,932.215 1160.5,1063.28 1181.43,1040.05 1202.36,940.243 1223.29,967.669 1244.21,960.785 \n",
       "  1265.14,1174.19 1286.07,1063.59 1307,933.989 1327.92,1137.06 1348.85,1058.24 1369.78,1203.83 1390.71,1172.13 1411.64,1214.2 1432.56,1008.48 1453.49,1086.38 \n",
       "  1474.42,1134.38 1495.35,1175.44 1516.27,1182.91 1537.2,1146.77 1558.13,1307.21 1579.06,1137.82 1599.98,1127.66 1620.91,1221.18 1641.84,1260.88 1662.77,1184.07 \n",
       "  1683.7,1243.09 1704.62,1228.6 1725.55,1328 1746.48,1270.79 1767.41,1192.52 1788.33,1269.77 1809.26,1404.33 1830.19,1186.46 1851.12,1352.2 1872.05,1236.76 \n",
       "  1892.97,1352.04 1913.9,1234.26 1934.83,1346.53 1955.76,1351.51 1976.68,1305.18 1997.61,1305.54 2018.54,1396.98 2039.47,1370.48 2060.4,1380.05 2081.32,1250.57 \n",
       "  2102.25,1445.72 2123.18,1386.26 2144.11,1398.08 2165.03,1406.8 2185.96,1389.04 2206.89,1335.97 2227.82,1358.53 2248.75,1427.29 2269.67,1444.43 2290.6,1358.9 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip480)\" d=\"\n",
       "M1950.74 198.898 L2279.55 198.898 L2279.55 95.2176 L1950.74 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#000000; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1950.74,198.898 2279.55,198.898 2279.55,95.2176 1950.74,95.2176 1950.74,198.898 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip480)\" style=\"stroke:#009af9; stroke-linecap:butt; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1975.14,147.058 2121.55,147.058 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip480)\" d=\"M2145.96 128.319 L2150.22 128.319 L2150.22 164.338 L2145.96 164.338 L2145.96 128.319 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M2169.17 141.398 Q2165.75 141.398 2163.76 144.083 Q2161.77 146.745 2161.77 151.398 Q2161.77 156.051 2163.73 158.736 Q2165.72 161.398 2169.17 161.398 Q2172.58 161.398 2174.57 158.713 Q2176.56 156.027 2176.56 151.398 Q2176.56 146.791 2174.57 144.106 Q2172.58 141.398 2169.17 141.398 M2169.17 137.787 Q2174.73 137.787 2177.9 141.398 Q2181.07 145.009 2181.07 151.398 Q2181.07 157.764 2177.9 161.398 Q2174.73 165.009 2169.17 165.009 Q2163.59 165.009 2160.42 161.398 Q2157.28 157.764 2157.28 151.398 Q2157.28 145.009 2160.42 141.398 Q2163.59 137.787 2169.17 137.787 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M2204.66 139.176 L2204.66 143.203 Q2202.85 142.277 2200.91 141.815 Q2198.96 141.352 2196.88 141.352 Q2193.71 141.352 2192.11 142.324 Q2190.54 143.296 2190.54 145.24 Q2190.54 146.722 2191.67 147.578 Q2192.81 148.412 2196.23 149.176 L2197.69 149.5 Q2202.23 150.472 2204.13 152.254 Q2206.05 154.014 2206.05 157.185 Q2206.05 160.796 2203.18 162.902 Q2200.33 165.009 2195.33 165.009 Q2193.25 165.009 2190.98 164.592 Q2188.73 164.199 2186.23 163.388 L2186.23 158.99 Q2188.59 160.217 2190.89 160.842 Q2193.18 161.444 2195.42 161.444 Q2198.43 161.444 2200.05 160.426 Q2201.67 159.384 2201.67 157.509 Q2201.67 155.773 2200.49 154.847 Q2199.34 153.921 2195.38 153.064 L2193.9 152.717 Q2189.94 151.884 2188.18 150.171 Q2186.42 148.435 2186.42 145.426 Q2186.42 141.768 2189.01 139.778 Q2191.6 137.787 2196.37 137.787 Q2198.73 137.787 2200.82 138.134 Q2202.9 138.481 2204.66 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip480)\" d=\"M2229.36 139.176 L2229.36 143.203 Q2227.55 142.277 2225.61 141.815 Q2223.66 141.352 2221.58 141.352 Q2218.41 141.352 2216.81 142.324 Q2215.24 143.296 2215.24 145.24 Q2215.24 146.722 2216.37 147.578 Q2217.51 148.412 2220.93 149.176 L2222.39 149.5 Q2226.93 150.472 2228.83 152.254 Q2230.75 154.014 2230.75 157.185 Q2230.75 160.796 2227.88 162.902 Q2225.03 165.009 2220.03 165.009 Q2217.95 165.009 2215.68 164.592 Q2213.43 164.199 2210.93 163.388 L2210.93 158.99 Q2213.29 160.217 2215.59 160.842 Q2217.88 161.444 2220.12 161.444 Q2223.13 161.444 2224.75 160.426 Q2226.37 159.384 2226.37 157.509 Q2226.37 155.773 2225.19 154.847 Q2224.03 153.921 2220.08 153.064 L2218.59 152.717 Q2214.64 151.884 2212.88 150.171 Q2211.12 148.435 2211.12 145.426 Q2211.12 141.768 2213.71 139.778 Q2216.3 137.787 2221.07 137.787 Q2223.43 137.787 2225.52 138.134 Q2227.6 138.481 2229.36 139.176 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "plot(loss_vector, label=\"loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JuliaLux 1.7.3",
   "language": "julia",
   "name": "julialux-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
